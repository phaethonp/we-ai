{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "pic2struct.ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phaethonp/we-ai/blob/main/pic2struct.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pix2Struct is an AI model that converts images to text, particularly focusing on understanding and interpreting visually-situated language. It learns from web screenshots, creating simplified HTML versions.\n",
        "\n",
        "Features of Pix2Struct:\n",
        "\n",
        "1. Pretrained on web screenshots: Uses the rich, diverse data available on the web for pretraining.\n",
        "\n",
        "2. Fine-tuned for specific tasks: Proven results on a variety of tasks, including image captioning, visual question answering, etc.\n",
        "\n",
        "3. Flexible language and vision input integration: Can overlay language prompts on the input image.\n",
        "\n",
        "4. Variable-resolution input representation: Capable of handling inputs of varying resolutions.\n",
        "\n",
        "5. Single model, multiple domains: Effective on tasks across different domains like documents, illustrations, user interfaces, and natural images.\n",
        "\n",
        "6. Contributed by open-source community: Maintained and improved by a global community of developers and researchers."
      ],
      "metadata": {
        "id": "RUAYPi3Z_Ll2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOnuRoQzrDFq"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\n",
        "from matplotlib import pyplot as plt\n",
        "import functools\n",
        "from google.colab import files\n",
        "import PIL\n"
      ],
      "metadata": {
        "id": "cT6fCanHrEfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload file here\n",
        "def upload_files():\n",
        "  uploaded = files.upload()\n",
        "  images = []\n",
        "  for k, v in uploaded.items():\n",
        "      open(k, 'wb').write(v)\n",
        "      images.append(PIL.Image.open(k))\n",
        "  return images\n",
        "images = upload_files()"
      ],
      "metadata": {
        "id": "SxiwZUedrFlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_urls = {\n",
        "    \"textcaps\": \"google/pix2struct-textcaps-large\", # Finetuned on TextCaps\n",
        "    \"screen2words\": \"google/pix2struct-screen2words-large\", # Finetuned on Screen2Words\n",
        "    \"widgetcaption\": \"google/pix2struct-widget-captioning-large\", # Finetuned on Widget Captioning (captioning a UI component on a screen)\n",
        "    \"infographics\": \"google/pix2struct-infographics-vqa-large\", # Infographics\n",
        "    \"docvqa\": \"google/pix2struct-docvqa-large\", # Visual question answering\n",
        "    \"ai2d\": \"google/pix2struct-ai2d-large\", # Scienfic diagram\n",
        "}\n",
        "\n",
        "models = {}\n",
        "def get_model(model_name):\n",
        "    if model_name not in models:\n",
        "        print(f\"Loading {model_name} from {model_urls[model_name]}\")\n",
        "        model = Pix2StructForConditionalGeneration.from_pretrained(model_urls[model_name]).to(\"cuda\")\n",
        "        processor = Pix2StructProcessor.from_pretrained(model_urls[model_name])\n",
        "        models[model_name] = (model, processor)\n",
        "    return models[model_name]\n",
        "\n",
        "def run_model(model_name, text=None):\n",
        "    text = text or \"where is the main button on this page?\"\n",
        "    model_url = model_urls[model_name]\n",
        "    model, processor = get_model(model_name)\n",
        "    if processor.image_processor.is_vqa:\n",
        "        print(f\"Adding prompt for VQA model: '{text}'\")\n",
        "        inputs = processor(images[0], return_tensors=\"pt\", text=text).to(\"cuda\")\n",
        "    else:\n",
        "        inputs = processor(images[0], return_tensors=\"pt\").to(\"cuda\")\n",
        "    predictions = model.generate(**inputs)\n",
        "    print(f\"Name: '{model_name}'\")\n",
        "    plt.imshow(images[0])\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "    print(f\"Output: '{processor.decode(predictions[0], skip_special_tokens=True)}'\")\n",
        "    "
      ],
      "metadata": {
        "id": "VPVmFWI8rJM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_model(\"screen2words\", text=\"what does this screen say?\")"
      ],
      "metadata": {
        "id": "BJjoLhmkrPME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_model(\"widgetcaption\", text=\"what are the buttons on this page?\")"
      ],
      "metadata": {
        "id": "KrV-xuanrPz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you run out of space, you might need to clear your RAM. I should probably switch the cache to some kind of LRU cache.\n",
        "del models\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "73W1YordrLSP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}